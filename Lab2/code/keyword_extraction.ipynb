{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re \n",
    "import operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from library import clean_text_simple, terms_to_graph, accuracy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# constants #\n",
    "#############\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stpwds = stopwords.words('english')\n",
    "punct = string.punctuation.replace('-', '')\n",
    "\n",
    "SWS = 4 # Sliding window size for k-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files processed\n",
      "50 files processed\n",
      "100 files processed\n",
      "150 files processed\n",
      "200 files processed\n",
      "250 files processed\n",
      "300 files processed\n",
      "350 files processed\n",
      "400 files processed\n",
      "450 files processed\n",
      "0 abstracts processed\n",
      "50 abstracts processed\n",
      "100 abstracts processed\n",
      "150 abstracts processed\n",
      "200 abstracts processed\n",
      "250 abstracts processed\n",
      "300 abstracts processed\n",
      "350 abstracts processed\n",
      "400 abstracts processed\n",
      "450 abstracts processed\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# read and pre-process abstracts #\n",
    "##################################\n",
    "\n",
    "path_to_abstracts = \"../data/Hulth2003testing/abstracts/\"\n",
    "abstract_names = sorted(os.listdir(path_to_abstracts))\n",
    "\n",
    "abstracts = []\n",
    "for counter,filename in enumerate(abstract_names):\n",
    "    # read file\n",
    "    with open(path_to_abstracts + '/' + filename, 'r') as my_file: \n",
    "        text = my_file.read().splitlines()\n",
    "    text = ' '.join(text)\n",
    "    # remove formatting\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    abstracts.append(text)\n",
    "    \n",
    "    if counter % round(len(abstract_names)/10) == 0:\n",
    "        print(counter, 'files processed')\n",
    "\n",
    "abstracts_cleaned = []\n",
    "for counter,abstract in enumerate(abstracts):\n",
    "    my_tokens = clean_text_simple(abstract,my_stopwords=stpwds,punct=punct)\n",
    "    abstracts_cleaned.append(my_tokens)\n",
    "    \n",
    "    if counter % round(len(abstracts)/10) == 0:\n",
    "        print(counter, 'abstracts processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files processed\n",
      "50 files processed\n",
      "100 files processed\n",
      "150 files processed\n",
      "200 files processed\n",
      "250 files processed\n",
      "300 files processed\n",
      "350 files processed\n",
      "400 files processed\n",
      "450 files processed\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# read and pre-process gold standard keywords #\n",
    "###############################################\n",
    "\n",
    "path_to_keywords = \"../data/Hulth2003testing/uncontr/\"\n",
    "keyword_names = sorted(os.listdir(path_to_keywords))\n",
    "\n",
    "keywords_gold_standard = []\n",
    "\n",
    "for counter,filename in enumerate(keyword_names):\n",
    "    # read file\n",
    "    with open(path_to_keywords + filename, 'r') as my_file: \n",
    "        text = my_file.read().splitlines()\n",
    "    text = ' '.join(text)\n",
    "    text =  re.sub('\\s+', ' ', text) # remove formatting\n",
    "    text = text.lower()\n",
    "    # turn string into list of keywords, preserving intra-word dashes \n",
    "    # but breaking n-grams into unigrams\n",
    "    keywords = text.split(';')\n",
    "    keywords = [keyword.strip().split(' ') for keyword in keywords]\n",
    "    keywords = [keyword for sublist in keywords for keyword in sublist] # flatten list\n",
    "    keywords = [keyword for keyword in keywords if keyword not in stpwds] # remove stopwords (rare but may happen due to n-gram breaking)\n",
    "    keywords_stemmed = [stemmer.stem(keyword) for keyword in keywords]\n",
    "    keywords_stemmed_unique = list(set(keywords_stemmed)) # remove duplicates (may happen due to n-gram breaking)\n",
    "    keywords_gold_standard.append(keywords_stemmed_unique)\n",
    "    \n",
    "    if counter % round(len(keyword_names)/10) == 0:\n",
    "        print(counter, 'files processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# precompute graphs-of-words #\n",
    "##################d############\n",
    "\n",
    "gs = [terms_to_graph(abstracts, w=SWS) for abstracts in abstracts_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# keyword extraction with k-core #\n",
    "##################################\n",
    "\n",
    "keywords_kc = []  \n",
    "\n",
    "for counter,g in enumerate(gs):\n",
    "    core_numbers = dict(zip(g.vs['name'],g.coreness())) # compute core numbers\n",
    "    # retain main core as keywords\n",
    "    max_c_n = max(core_numbers.values())\n",
    "    keywords = [kwd for kwd,c_n in core_numbers.items() if c_n==max_c_n]\n",
    "    keywords_kc.append(keywords)\n",
    "    \n",
    "    if counter % round(len(gs)/10) == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# keyword extraction with PageRank #\n",
    "####################################\n",
    "\n",
    "my_percentage = 0.33\n",
    "\n",
    "keywords_pr = []\n",
    "\n",
    "for counter,g in enumerate(gs):\n",
    "    pr_scores = g.pagerank()\n",
    "    pr_scores = sorted(zip(pr_scores,g.vs['name']), reverse=True) # rank in decreasing order\n",
    "\n",
    "    # retain top my_percentage words as keywords\n",
    "    top = int(my_percentage*len(pr_scores)) \n",
    "    keywords = [k for _,k in pr_scores[:top]]\n",
    "    keywords_pr.append(keywords)  \n",
    "\n",
    "    if counter % round(len(gs)/10) == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# keyword extraction with TF-IDF #\n",
    "##################################\n",
    "\n",
    "# to ensure same pre-processing as the other methods\n",
    "abstracts_cleaned_strings = [' '.join(elt) for elt in abstracts_cleaned]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stpwds) # use TfidfVectorizer passing 'stpwds' as stopwords\n",
    "doc_term_matrix = tfidf_vectorizer.fit_transform(abstracts_cleaned_strings)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "vectors_list = doc_term_matrix.todense().tolist()\n",
    "\n",
    "keywords_tfidf = []\n",
    "\n",
    "for counter,vector in enumerate(vectors_list):\n",
    "    terms_weights = zip(terms,vector) # bow feature vector as list of tuples\n",
    "\n",
    "    nonzero = [tuple((t,w)) for t,w in terms_weights if w != 0] # keep only non zero values (the words in the document) and save as object 'nonzero'\n",
    "    nonzero = sorted(nonzero, key=operator.itemgetter(1), reverse=True) # rank by decreasing weights\n",
    "\n",
    "    numb_to_retain = int(len(nonzero)*my_percentage) # retain top 'my_percentage' % words as keywords\n",
    "    keywords = [pair[0] for pair in nonzero[:numb_to_retain]]\n",
    "    keywords_tfidf.append(keywords)\n",
    "    \n",
    "    if counter % round(len(vectors_list)/10) == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-core performance: \n",
      "\n",
      "precision: 51.86\n",
      "recall: 62.56\n",
      "F-1 score: 51.55\n",
      "\n",
      "\n",
      "tfidf performance: \n",
      "\n",
      "precision: 59.21\n",
      "recall: 38.5\n",
      "F-1 score: 44.85\n",
      "\n",
      "\n",
      "PageRank performance: \n",
      "\n",
      "precision: 60.16\n",
      "recall: 38.3\n",
      "F-1 score: 44.96\n",
      "\n",
      "\n",
      "k-core seems to give a better recall but lower precision than tf-idf and pagerank: in overall,\n",
      "its f1-score is better, but it could surely be improved by optimizing the sliding window size\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# performance comparison #\n",
    "##########################\n",
    "\n",
    "perf_kc = []\n",
    "perf_tfidf = []\n",
    "perf_pr = []\n",
    "\n",
    "for idx, truth in enumerate(keywords_gold_standard):\n",
    "    # use the 'accuracy_metrics' function\n",
    "\n",
    "    perf_kc.append(accuracy_metrics(keywords_kc[idx], truth))\n",
    "    perf_tfidf.append(accuracy_metrics(keywords_tfidf[idx], truth))\n",
    "    perf_pr.append(accuracy_metrics(keywords_pr[idx], truth))\n",
    "\n",
    "lkgs = len(keywords_gold_standard)\n",
    "\n",
    "# print macro-averaged results (averaged at the collection level)\n",
    "results = {'k-core':perf_kc,'tfidf':perf_tfidf,'PageRank':perf_pr}\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(name + ' performance: \\n')\n",
    "    print('precision:', round(100*sum([tuple[0] for tuple in result])/lkgs,2))\n",
    "    print('recall:', round(100*sum([tuple[1] for tuple in result])/lkgs,2))\n",
    "    print('F-1 score:', round(100*sum([tuple[2] for tuple in result])/lkgs,2))\n",
    "    print('\\n')\n",
    "    \n",
    "print(\"\"\"k-core seems to give a better recall but lower precision than tf-idf and pagerank: in overall,\n",
    "its f1-score is better, but it could surely be improved by optimizing the sliding window size\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
