{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.backend.tensorflow_backend import _to_tensor\n",
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, GRU, TimeDistributed, Dense\n",
    "\n",
    "from AttentionWithContext import AttentionWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = ''\n",
    "path_to_data = path_root + 'data/'\n",
    "\n",
    "sys.path.insert(0, path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidir_gru(my_seq,n_units):\n",
    "    '''\n",
    "    just a convenient wrapper for bidirectional RNN with GRU units\n",
    "    '''\n",
    "    \n",
    "    # add a default GRU layer (https://keras.io/layers/recurrent/). You need to specify only the 'units' and 'return_sequences' arguments\n",
    "    return Bidirectional(GRU(n_units, return_sequences=True),\n",
    "                         merge_mode='concat', weights=None)(my_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = parameters = = = = =\n",
    "\n",
    "n_units   = 50\n",
    "drop_rate = 0.5 \n",
    "mfw_idx   = 2 # index of the most frequent words in the dictionary. \n",
    "              # 0 is for the special padding token\n",
    "              # 1 is for the special out-of-vocabulary token\n",
    "\n",
    "padding_idx  = 0\n",
    "oov_idx      = 1\n",
    "batch_size   = 32\n",
    "nb_epochs    = 6\n",
    "my_optimizer = 'adam'\n",
    "my_patience  = 2 # for early stopping strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = data loading = = = = =\n",
    "\n",
    "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
    "my_docs_array_test  = np.load(path_to_data + 'docs_test.npy')\n",
    "\n",
    "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
    "my_labels_array_test  = np.load(path_to_data + 'labels_test.npy')\n",
    "\n",
    "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
    "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
    "    word_to_index = json.load(my_file)\n",
    "\n",
    "# invert mapping\n",
    "index_to_word = dict((v,k) for k,v in word_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created\n",
      "embeddings compressed\n"
     ]
    }
   ],
   "source": [
    "# = = = = = loading pretrained word vectors = = = = =\n",
    "\n",
    "wvs = KeyedVectors.load(path_to_data + 'word_vectors.kv', mmap='r')\n",
    "\n",
    "assert len(wvs.wv.vocab) == len(word_to_index) + 1 # vocab does not contain the OOV token\n",
    "\n",
    "word_vecs = wvs.wv.syn0\n",
    "\n",
    "pad_vec = np.random.normal(size=word_vecs.shape[1])\n",
    "\n",
    "# add Gaussian vector on top of embedding matrix (padding vector)\n",
    "word_vecs = np.insert(word_vecs,0,pad_vec,0)\n",
    "\n",
    "print('embeddings created')\n",
    "\n",
    "# reduce dimension with PCA (to reduce the number of parameters of the model)\n",
    "my_pca = PCA(n_components=64)\n",
    "embeddings_pca = my_pca.fit_transform(word_vecs)\n",
    "\n",
    "print('embeddings compressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = defining architecture = = = = =\n",
    "\n",
    "# = = = sentence encoder\n",
    "\n",
    "sent_ints = Input(shape=(my_docs_array_train.shape[2],)) # vec of ints of variable size\n",
    "\n",
    "sent_wv = Embedding(input_dim=embeddings_pca.shape[0], # vocab size\n",
    "                    output_dim=embeddings_pca.shape[1], # dimensionality of embedding space\n",
    "                    weights=[embeddings_pca],\n",
    "                    input_length=my_docs_array_train.shape[2],\n",
    "                    trainable=True\n",
    "                    )(sent_ints)\n",
    "\n",
    "sent_wv_dr = Dropout(drop_rate)(sent_wv)\n",
    "\n",
    "# use bidir_gru, AttentionWithContext with return_coefficients=True, and Dropout\n",
    "# warning: AttentionWithContext will return a list of two objects!\n",
    "sent_gru = bidir_gru(sent_wv_dr, n_units)\n",
    "sent_att_w_ctx, word_att_coeffs = AttentionWithContext(return_coefficients=True)(sent_gru)\n",
    "sent_att_vec_dr = Dropout(drop_rate)(sent_att_w_ctx)\n",
    "\n",
    "sent_encoder = Model(sent_ints,sent_att_vec_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n"
     ]
    }
   ],
   "source": [
    "# = = = document encoder\n",
    "\n",
    "doc_ints = Input(shape=(my_docs_array_train.shape[1],my_docs_array_train.shape[2],))\n",
    "\n",
    "### fill the gap (4 gaps) ###\n",
    "# use TimeDistributed (https://keras.io/layers/wrappers/), bidir_gru, AttentionWithContext with return_coefficients=True, and Dropout\n",
    "# warning: AttentionWithContext will return a list of two objects!\n",
    "\n",
    "doc_td = TimeDistributed(sent_encoder)(doc_ints)\n",
    "doc_gru = bidir_gru(doc_td, n_units)\n",
    "doc_att_w_ctx, sent_att_coeffs = AttentionWithContext(return_coefficients=True)(doc_gru)\n",
    "doc_att_vec_dr = Dropout(drop_rate)(doc_att_w_ctx)\n",
    "\n",
    "preds = Dense(units=1, activation='sigmoid')(doc_att_vec_dr)\n",
    "\n",
    "model = Model(doc_ints,preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = my_optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print('model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.5487 - acc: 0.7041 - val_loss: 0.3909 - val_acc: 0.8246\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39089, saving model to data/model\n",
      "Epoch 2/6\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.3601 - acc: 0.8443 - val_loss: 0.3568 - val_acc: 0.8414\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39089 to 0.35683, saving model to data/model\n",
      "Epoch 3/6\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.2754 - acc: 0.8848 - val_loss: 0.3588 - val_acc: 0.8428\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35683\n",
      "Epoch 4/6\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.2274 - acc: 0.9081 - val_loss: 0.3868 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35683\n",
      "Epoch 5/6\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.1782 - acc: 0.9310 - val_loss: 0.4117 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35683\n"
     ]
    }
   ],
   "source": [
    "# = = = = = training = = = = =\n",
    "\n",
    "loading_pretrained = False\n",
    "\n",
    "if not loading_pretrained:\n",
    "    \n",
    "    # go through epochs as long as accuracy on validation set increases\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=my_patience, mode='max')\n",
    "    \n",
    "    # save model corresponding to best epoch\n",
    "    checkpointer = ModelCheckpoint(filepath=path_to_data + 'model', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    # 200s/epoch on CPU - reaches 84.38% accuracy in 2 epochs\n",
    "    model.fit(my_docs_array_train, \n",
    "              my_labels_array_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (my_docs_array_test,my_labels_array_test),\n",
    "              callbacks = [early_stopping,checkpointer])\n",
    "\n",
    "else:\n",
    "    model.load_weights(path_to_data + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = extraction of attention coefficients = = = = =\n",
    "\n",
    "# define intermediate models: in each case, use the right inputs, and as outputs, the coefficients returned by the corresponding AttentionWithContext layer\n",
    "get_word_att_coeffs = Model(sent_ints, word_att_coeffs) # extracts the attention coefficients of the words in a sentence\n",
    "get_sent_att_coeffs = Model(doc_ints, sent_att_coeffs) # extracts the attention coefficients over the sentences in a document\n",
    "\n",
    "my_review = my_docs_array_test[-1:,:,:] # select last review\n",
    "# convert integer review to text\n",
    "index_to_word[1] = 'OOV'\n",
    "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.57 There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )\n",
      "7.9 Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot\n",
      "13.94 As others have pointed out , one single viewing of this movie is not sufficient .\n",
      "10.35 If you have the DVD of MD , you can OOV ' by looking at David Lynch 's 'Top 10 OOV to OOV MD ' ( but only upon second\n",
      "16.61 ; ) First of all , Mulholland Drive is downright brilliant .\n",
      "21.24 A masterpiece .\n",
      "21.38 This is the kind of movie that refuse to leave your head .\n"
     ]
    }
   ],
   "source": [
    "# = = = attention over sentences in the document\n",
    "\n",
    "sent_coeffs = get_sent_att_coeffs.predict(my_review)\n",
    "sent_coeffs = sent_coeffs[0,:,:]\n",
    "\n",
    "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
    "    print(round(elt[0]*100,2),elt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There', 0.26)\n",
      "(\"'s\", 0.24)\n",
      "('a', 0.28)\n",
      "('sign', 0.28)\n",
      "('on', 0.29)\n",
      "('The', 0.28)\n",
      "('Lost', 0.32)\n",
      "('Highway', 0.23)\n",
      "('that', 0.2)\n",
      "('says', 0.15)\n",
      "(':', 0.21)\n",
      "('OOV', 0.23)\n",
      "('SPOILERS', 0.28)\n",
      "('OOV', 0.3)\n",
      "('(', 0.23)\n",
      "('but', 0.23)\n",
      "('you', 0.29)\n",
      "('already', 0.18)\n",
      "('knew', 0.16)\n",
      "('that', 0.2)\n",
      "(',', 0.25)\n",
      "('did', 0.15)\n",
      "(\"n't\", 0.19)\n",
      "('you', 0.34)\n",
      "('?', 0.36)\n",
      "(')', 0.43)\n",
      "= = = =\n",
      "('Since', 0.15)\n",
      "('there', 0.38)\n",
      "(\"'s\", 0.33)\n",
      "('a', 0.34)\n",
      "('great', 0.16)\n",
      "('deal', 0.15)\n",
      "('of', 0.21)\n",
      "('people', 0.27)\n",
      "('that', 0.25)\n",
      "('apparently', 0.23)\n",
      "('did', 0.13)\n",
      "('not', 0.21)\n",
      "('get', 0.23)\n",
      "('the', 0.28)\n",
      "('point', 0.35)\n",
      "('of', 0.32)\n",
      "('this', 0.33)\n",
      "('movie', 0.35)\n",
      "(',', 0.36)\n",
      "('I', 0.36)\n",
      "(\"'d\", 0.24)\n",
      "('like', 0.18)\n",
      "('to', 0.25)\n",
      "('contribute', 0.18)\n",
      "('my', 0.27)\n",
      "('interpretation', 0.26)\n",
      "('of', 0.27)\n",
      "('why', 0.31)\n",
      "('the', 0.29)\n",
      "('plot', 0.3)\n",
      "= = = =\n",
      "('As', 0.19)\n",
      "('others', 0.17)\n",
      "('have', 0.18)\n",
      "('pointed', 0.23)\n",
      "('out', 0.34)\n",
      "(',', 0.32)\n",
      "('one', 0.28)\n",
      "('single', 0.19)\n",
      "('viewing', 0.07)\n",
      "('of', 0.18)\n",
      "('this', 0.25)\n",
      "('movie', 0.32)\n",
      "('is', 0.43)\n",
      "('not', 0.49)\n",
      "('sufficient', 0.56)\n",
      "('.', 0.65)\n",
      "= = = =\n",
      "('If', 0.28)\n",
      "('you', 0.39)\n",
      "('have', 0.28)\n",
      "('the', 0.3)\n",
      "('DVD', 0.19)\n",
      "('of', 0.28)\n",
      "('MD', 0.3)\n",
      "(',', 0.36)\n",
      "('you', 0.46)\n",
      "('can', 0.47)\n",
      "('OOV', 0.4)\n",
      "(\"'\", 0.35)\n",
      "('by', 0.32)\n",
      "('looking', 0.43)\n",
      "('at', 0.34)\n",
      "('David', 0.4)\n",
      "('Lynch', 0.19)\n",
      "(\"'s\", 0.35)\n",
      "(\"'Top\", 0.32)\n",
      "('10', 0.34)\n",
      "('OOV', 0.38)\n",
      "('to', 0.46)\n",
      "('OOV', 0.41)\n",
      "('MD', 0.38)\n",
      "(\"'\", 0.35)\n",
      "('(', 0.29)\n",
      "('but', 0.28)\n",
      "('only', 0.24)\n",
      "('upon', 0.39)\n",
      "('second', 0.39)\n",
      "= = = =\n",
      "(';', 0.27)\n",
      "(')', 0.2)\n",
      "('First', 0.23)\n",
      "('of', 0.24)\n",
      "('all', 0.26)\n",
      "(',', 0.31)\n",
      "('Mulholland', 0.31)\n",
      "('Drive', 0.25)\n",
      "('is', 0.38)\n",
      "('downright', 0.3)\n",
      "('brilliant', 0.32)\n",
      "('.', 0.53)\n",
      "= = = =\n",
      "('A', 0.24)\n",
      "('masterpiece', 0.57)\n",
      "('.', 0.75)\n",
      "= = = =\n",
      "('This', 0.35)\n",
      "('is', 0.4)\n",
      "('the', 0.41)\n",
      "('kind', 0.36)\n",
      "('of', 0.33)\n",
      "('movie', 0.38)\n",
      "('that', 0.33)\n",
      "('refuse', 0.22)\n",
      "('to', 0.39)\n",
      "('leave', 0.53)\n",
      "('your', 0.76)\n",
      "('head', 1.21)\n",
      "('.', 1.07)\n",
      "= = = =\n",
      "(')', 0.43)\n",
      "('?', 0.36)\n",
      "('you', 0.34)\n",
      "('Lost', 0.32)\n",
      "('OOV', 0.3)\n",
      "('on', 0.29)\n",
      "('you', 0.29)\n",
      "('a', 0.28)\n",
      "('sign', 0.28)\n",
      "('The', 0.28)\n",
      "('SPOILERS', 0.28)\n",
      "('There', 0.26)\n",
      "(',', 0.25)\n",
      "(\"'s\", 0.24)\n",
      "('Highway', 0.23)\n",
      "('OOV', 0.23)\n",
      "('(', 0.23)\n",
      "('but', 0.23)\n",
      "(':', 0.21)\n",
      "('that', 0.2)\n",
      "('that', 0.2)\n",
      "(\"n't\", 0.19)\n",
      "('already', 0.18)\n",
      "('knew', 0.16)\n",
      "('says', 0.15)\n",
      "('did', 0.15)\n",
      "= = = =\n",
      "('there', 0.38)\n",
      "(',', 0.36)\n",
      "('I', 0.36)\n",
      "('point', 0.35)\n",
      "('movie', 0.35)\n",
      "('a', 0.34)\n",
      "(\"'s\", 0.33)\n",
      "('this', 0.33)\n",
      "('of', 0.32)\n",
      "('why', 0.31)\n",
      "('plot', 0.3)\n",
      "('the', 0.29)\n",
      "('the', 0.28)\n",
      "('people', 0.27)\n",
      "('my', 0.27)\n",
      "('of', 0.27)\n",
      "('interpretation', 0.26)\n",
      "('that', 0.25)\n",
      "('to', 0.25)\n",
      "(\"'d\", 0.24)\n",
      "('apparently', 0.23)\n",
      "('get', 0.23)\n",
      "('of', 0.21)\n",
      "('not', 0.21)\n",
      "('like', 0.18)\n",
      "('contribute', 0.18)\n",
      "('great', 0.16)\n",
      "('Since', 0.15)\n",
      "('deal', 0.15)\n",
      "('did', 0.13)\n",
      "= = = =\n",
      "('.', 0.65)\n",
      "('sufficient', 0.56)\n",
      "('not', 0.49)\n",
      "('is', 0.43)\n",
      "('out', 0.34)\n",
      "(',', 0.32)\n",
      "('movie', 0.32)\n",
      "('one', 0.28)\n",
      "('this', 0.25)\n",
      "('pointed', 0.23)\n",
      "('As', 0.19)\n",
      "('single', 0.19)\n",
      "('have', 0.18)\n",
      "('of', 0.18)\n",
      "('others', 0.17)\n",
      "('viewing', 0.07)\n",
      "= = = =\n",
      "('can', 0.47)\n",
      "('you', 0.46)\n",
      "('to', 0.46)\n",
      "('looking', 0.43)\n",
      "('OOV', 0.41)\n",
      "('OOV', 0.4)\n",
      "('David', 0.4)\n",
      "('you', 0.39)\n",
      "('upon', 0.39)\n",
      "('second', 0.39)\n",
      "('OOV', 0.38)\n",
      "('MD', 0.38)\n",
      "(',', 0.36)\n",
      "(\"'\", 0.35)\n",
      "(\"'s\", 0.35)\n",
      "(\"'\", 0.35)\n",
      "('at', 0.34)\n",
      "('10', 0.34)\n",
      "('by', 0.32)\n",
      "(\"'Top\", 0.32)\n",
      "('the', 0.3)\n",
      "('MD', 0.3)\n",
      "('(', 0.29)\n",
      "('If', 0.28)\n",
      "('have', 0.28)\n",
      "('of', 0.28)\n",
      "('but', 0.28)\n",
      "('only', 0.24)\n",
      "('DVD', 0.19)\n",
      "('Lynch', 0.19)\n",
      "= = = =\n",
      "('.', 0.53)\n",
      "('is', 0.38)\n",
      "('brilliant', 0.32)\n",
      "(',', 0.31)\n",
      "('Mulholland', 0.31)\n",
      "('downright', 0.3)\n",
      "(';', 0.27)\n",
      "('all', 0.26)\n",
      "('Drive', 0.25)\n",
      "('of', 0.24)\n",
      "('First', 0.23)\n",
      "(')', 0.2)\n",
      "= = = =\n",
      "('.', 0.75)\n",
      "('masterpiece', 0.57)\n",
      "('A', 0.24)\n",
      "= = = =\n",
      "('head', 1.21)\n",
      "('.', 1.07)\n",
      "('your', 0.76)\n",
      "('leave', 0.53)\n",
      "('the', 0.41)\n",
      "('is', 0.4)\n",
      "('to', 0.39)\n",
      "('movie', 0.38)\n",
      "('kind', 0.36)\n",
      "('This', 0.35)\n",
      "('of', 0.33)\n",
      "('that', 0.33)\n",
      "('refuse', 0.22)\n",
      "= = = =\n"
     ]
    }
   ],
   "source": [
    "# = = = attention over words in each sentence\n",
    "\n",
    "my_review_tensor = _to_tensor(my_review,dtype='float32') # a layer, unlike a model, requires a TensorFlow tensor as input\n",
    "\n",
    "# apply the 'get_word_att_coeffs' model over all the sentences in 'my_review_tensor'\n",
    "word_coeffs = TimeDistributed(get_word_att_coeffs)(my_review_tensor)\n",
    "\n",
    "word_coeffs = K.eval(word_coeffs) # shape = (7, 30, 1): (batch size, nb of sents in doc, nb of words per sent, coeff)\n",
    "\n",
    "word_coeffs = word_coeffs[0,:,:,0] # shape = (7, 30) (coeff for each word in each sentence)\n",
    "\n",
    "word_coeffs = sent_coeffs * word_coeffs # re-weigh according to sentence importance\n",
    "\n",
    "word_coeffs = np.round((word_coeffs*100).astype(np.float64),2)\n",
    "\n",
    "word_coeffs_list = word_coeffs.tolist()\n",
    "\n",
    "# match text and coefficients\n",
    "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
    "\n",
    "for sent in text_word_coeffs:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')\n",
    "\n",
    "# sort words by importance within each sentence\n",
    "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
    "\n",
    "for sent in text_word_coeffs_sorted:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
