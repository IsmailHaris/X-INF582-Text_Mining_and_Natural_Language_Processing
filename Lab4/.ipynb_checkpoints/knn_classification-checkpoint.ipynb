{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(111417)\n",
    "\n",
    "# remove dashes and apostrophes from punctuation marks \n",
    "punct = string.punctuation.replace('-', '').replace(\"'\",'')\n",
    "# regex to match intra-word dashes and intra-word apostrophes\n",
    "my_regex = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "\n",
    "path_root = \"\"\n",
    "\n",
    "path_to_data = path_root + 'data/'\n",
    "path_to_documents = path_root + 'data/documents/'\n",
    "path_to_google_news = path_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "# returns the vector of a word\n",
    "def my_vector_getter(word, wv):\n",
    "    try:\n",
    "        # we use reshape because cosine similarity in sklearn now works only for multidimensional arrays\n",
    "        word_array = wv.wv[word].reshape(1,-1)\n",
    "        return (word_array)\n",
    "    except KeyError:\n",
    "        print('word: <', word, '> not in vocabulary!')\n",
    "    \n",
    "# performs basic pre-processing\n",
    "def clean_string(my_str, punct=punct, my_regex=my_regex, to_lower=False):\n",
    "    if to_lower:\n",
    "        my_str = my_str.lower()\n",
    "    # remove formatting\n",
    "    my_str = re.sub('\\s+', ' ', my_str)\n",
    "     # remove punctuation\n",
    "    my_str = ''.join(l for l in my_str if l not in punct)\n",
    "    # remove dashes that are not intra-word\n",
    "    my_str = my_regex.sub(lambda x: (x.group(1) if x.group(1) else ' '), my_str)\n",
    "    # strip extra white space\n",
    "    my_str = re.sub(' +',' ', my_str)\n",
    "    # strip leading and trailing white space\n",
    "    my_str = my_str.strip()\n",
    "    return my_str\n",
    "\n",
    "# append the Word Mover's Distance between doc and doc_train\n",
    "def to_parallelize(doc,collection,w2v):\n",
    "    to_return = []\n",
    "    for doc_train in collection:\n",
    "        to_return.append(w2v.wv.wmdistance(doc,doc_train))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1131\n",
      "2262\n",
      "3393\n",
      "4524\n",
      "5655\n",
      "6786\n",
      "7917\n",
      "9048\n",
      "10179\n",
      "11310\n",
      "documents, labels and stopwords loaded in 3.13 second(s)\n",
      "documents and labels shuffled\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "with open(path_to_data + 'smart_stopwords.txt', 'r') as my_file: \n",
    "    stpwds = my_file.read().splitlines()\n",
    "\n",
    "doc_names = os.listdir(path_to_documents)\n",
    "doc_names.sort(key=natural_keys)\n",
    "docs = []\n",
    "for idx,name in enumerate(doc_names):\n",
    "    with open(path_to_documents + name,'r') as my_file:\n",
    "        docs.append(my_file.read())\n",
    "    if idx % round(len(doc_names)/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "with open(path_to_data + 'labels.txt', 'r') as my_file: \n",
    "    labels = my_file.read().splitlines()\n",
    "\n",
    "labels = np.array([int(item) for item in labels])\n",
    "\n",
    "print('documents, labels and stopwords loaded in', round(time.time() - t,2), 'second(s)')\n",
    "\n",
    "shuffled_idxs = random.sample(range(len(docs)), len(docs)) # sample w/o replct\n",
    "docs = [docs[idx] for idx in shuffled_idxs]\n",
    "labels = [labels[idx] for idx in shuffled_idxs]\n",
    "\n",
    "print('documents and labels shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1131\n",
      "2262\n",
      "3393\n",
      "4524\n",
      "5655\n",
      "6786\n",
      "7917\n",
      "9048\n",
      "10179\n",
      "11310\n",
      "documents cleaned in 53.03 second(s)\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "cleaned_docs = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    # clean\n",
    "    doc = clean_string(doc, punct, my_regex, to_lower=True)\n",
    "    # tokenize (split based on whitespace)\n",
    "    tokens = doc.split(' ')\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stpwds]\n",
    "    # remove digits\n",
    "    tokens = [''.join([elt for elt in token if not elt.isdigit()]) for token in tokens]\n",
    "    # remove tokens shorter than 3 characters in size\n",
    "    tokens = [token for token in tokens if len(token)>2]\n",
    "    # remove tokens exceeding 25 characters in size\n",
    "    tokens = [token for token in tokens if len(token)<=25]\n",
    "    cleaned_docs.append(tokens)\n",
    "    if idx % round(len(docs)/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('documents cleaned in', round(time.time() - t,2), 'second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors loaded in 184.0 second(s)\n"
     ]
    }
   ],
   "source": [
    "# create empty word vectors for the words in vocabulary \n",
    "# we set size=300 to match dim of GNews word vectors\n",
    "my_q = 300\n",
    "mcount = 5\n",
    "w2v = Word2Vec(size=my_q, min_count=mcount)\n",
    "\n",
    "w2v.build_vocab(cleaned_docs)\n",
    "\n",
    "vocab = list(w2v.wv.vocab.keys()) # w2v.wv.vocab returns a dictionary\n",
    "all_tokens = [token for sublist in cleaned_docs for token in sublist]\n",
    "t_counts = dict(Counter(all_tokens))\n",
    "assert len(vocab) == len([token for token,count in t_counts.items() if count>=mcount])\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "w2v.intersect_word2vec_format(path_to_google_news + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "print('word vectors loaded in', round(time.time() - t,2), 'second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1131\n",
      "2262\n",
      "3393\n",
      "4524\n",
      "5655\n",
      "6786\n",
      "7917\n",
      "9048\n",
      "10179\n",
      "11310\n",
      "documents truncated to 100 word(s)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: in-vocab words without an entry in the Google News file are not removed from the vocabulary\n",
    "# instead, their vectors are silently initialized to random values\n",
    "# we can detect those vectors via their norms which approach zero\n",
    "norms = [np.linalg.norm(w2v[word]) for word in vocab]\n",
    "idxs_zero_norms = [idx for idx,norm in enumerate(norms) if norm<=0.05]\n",
    "# get the words with close to zero norms\n",
    "no_entry_words = [vocab[idx] for idx in idxs_zero_norms]\n",
    "\n",
    "# remove no-entry words and infrequent words\n",
    "no_entry_words = set(no_entry_words)\n",
    "for idx,doc in enumerate(cleaned_docs):\n",
    "    cleaned_docs[idx] = [token for token in doc if token not in no_entry_words and t_counts[token]>=mcount]\n",
    "    if idx % round(len(docs)/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "# retain only 'max_size' first words of each doc to speed-up computation of WMD\n",
    "max_size = 100\n",
    "\n",
    "cleaned_docs = [elt[:max_size] for elt in cleaned_docs]\n",
    "\n",
    "print('documents truncated to', max_size, 'word(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1131\n",
      "2262\n",
      "3393\n",
      "4524\n",
      "5655\n",
      "6786\n",
      "7917\n",
      "9048\n",
      "10179\n",
      "11310\n",
      "centroids computed in 4.44 second(s)\n"
     ]
    }
   ],
   "source": [
    "# compute centroids of documents\n",
    "t = time.time()\n",
    "\n",
    "centroids = np.empty(shape=(len(cleaned_docs),my_q))\n",
    "for idx,doc in enumerate(cleaned_docs):\n",
    "    # compute the centroid by using mean and concatenate\n",
    "    centroid = np.mean(np.concatenate([my_vector_getter(word, w2v) for word in doc]), axis=0)\n",
    "    centroids[idx,:] = centroid\n",
    "    if idx % round(len(docs)/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('centroids computed in', round(time.time() - t,2), 'second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 100 documents as examples\n",
      "using 50 documents for testing\n"
     ]
    }
   ],
   "source": [
    "# use the first n_train docs as training set and last n_test docs as test set\n",
    "# compute distance between each element in the test set and each element in the training set\n",
    "\n",
    "n_train = 100\n",
    "n_test = 50\n",
    "\n",
    "print('using', n_train, 'documents as examples')\n",
    "print('using', n_test, 'documents for testing')\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(min_df=1, stop_words=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "# tfidf_vectorizer takes raw documents as input\n",
    "doc_term_mtx = tfidf_vect.fit_transform([' '.join(elt) for elt in cleaned_docs[:n_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "TFIDF cosine similarities computed in 0.12 second(s)\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "my_similarities = []\n",
    "for idx,doc_test in enumerate(cleaned_docs[-n_test:]):\n",
    "    # notice that we just transform\n",
    "    doc_test_vect = tfidf_vect.transform([' '.join(doc_test)])\n",
    "    sims = cosine(doc_term_mtx, Y=doc_test_vect, dense_output=True)\n",
    "    my_similarities.append(sims[:,0])\n",
    "    if idx % round(n_test/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('TFIDF cosine similarities computed in', round(time.time() - t,2), 'second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "centroid-based cosine similarities computed in 0.06 second(s)\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "my_centroid_similarities = []\n",
    "for idx in range(n_test):\n",
    "    sims = cosine(centroids[:n_train,:], \n",
    "                  Y=centroids[centroids.shape[0]-(idx+1),:].reshape(1, -1), \n",
    "                  dense_output=True)\n",
    "    my_centroid_similarities.append(sims[:,0])\n",
    "    if idx % round(n_test/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('centroid-based cosine similarities computed in', round(time.time() - t,2), 'second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 8 core(s)\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "to_parallelize_partial = partial(to_parallelize, collection=cleaned_docs[:n_train], w2v=w2v)\n",
    " \n",
    "n_jobs = multiprocessing.cpu_count()\n",
    "\n",
    "print('using', n_jobs, 'core(s)')\n",
    "pool = Pool(processes=n_jobs)\n",
    "my_distances = pool.map(to_parallelize_partial, cleaned_docs[-n_test:])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# use the commands below if parallelization does not work\n",
    "'''\n",
    "my_distances = []\n",
    "for idx,doc_test in enumerate(cleaned_docs[-n_test:]):\n",
    "    tmp = []\n",
    "    for doc_train in cleaned_docs[:n_train]:\n",
    "        tmp.append(w2v.wv.wmdistance(doc_test,doc_train))\n",
    "    my_distances.append(tmp)\n",
    "    if idx % round(n_test/10) == 0:\n",
    "        print(idx)\n",
    "'''\n",
    "\n",
    "print('WM distances computed in', round(time.time() - t,2), 'second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('========== performance of WMD ==========')\n",
    "\n",
    "for nn in [1,3,5,7,11,13,15,17,21,23]:\n",
    "    \n",
    "    preds_wmd = []\n",
    "    for idx,dists in enumerate(my_distances):\n",
    "        idxs_sorted = np.argsort(dists).tolist() # by default, indexes of the elements sorted by increasing order!\n",
    "        # get labels of 'nn' nearest neighbors\n",
    "        labels_nn = [labels[:n_train][elt] for elt in idxs_sorted[:nn]] # the less distant elements are the closest\n",
    "        # select most frequent label as prediction\n",
    "        counts = dict(Counter(labels_nn))\n",
    "        max_counts = max(list(counts.values()))\n",
    "        pred = [k for k,v in counts.items() if v==max_counts][0]\n",
    "        preds_wmd.append(pred)\n",
    "    \n",
    "    # compare predictions to true labels\n",
    "    \n",
    "    print('accuracy for',nn,'nearest neighbors:',accuracy_score(labels[-n_test:],preds_wmd))\n",
    "\n",
    "print('========== performance of centroids ==========')\n",
    "\n",
    "for nn in [1,3,5,7,11,13,15,17,21,23]:\n",
    "    \n",
    "    preds_centroids = []\n",
    "    for idx,sims in enumerate(my_centroid_similarities):\n",
    "        idxs_sorted = np.argsort(sims).tolist()\n",
    "        # get labels of 'nn' nearest neighbors. Be cautious about the difference between distance and similarity!\n",
    "        labels_nn = [labels[:n_train][elt] for elt in idxs_sorted[nn:]] # the most elements with greater similarity are the closest\n",
    "        # select most frequent label as prediction\n",
    "        counts = dict(Counter(labels_nn))\n",
    "        max_counts = max(list(counts.values()))\n",
    "        pred = [k for k,v in counts.items() if v==max_counts][0]\n",
    "        preds_centroids.append(pred)\n",
    "    \n",
    "    # compare predictions to true labels\n",
    "    \n",
    "    print('accuracy for',nn,'nearest neighbors:',accuracy_score(labels[-n_test:],preds_centroids))\n",
    "\n",
    "\n",
    "print('========== performance of TFIDF ==========')\n",
    "\n",
    "for nn in [1,3,5,7,11,13,15,17,21,23]:\n",
    "    \n",
    "    preds_tfidf = []\n",
    "    for idx,sims in enumerate(my_similarities):\n",
    "        # sort by decreasing order\n",
    "        idxs_sorted = np.argsort(sims).tolist()\n",
    "        ### fill gap ### get labels of 'nn' nearest neighbors. Be cautious about the difference between distance and similarity!\n",
    "        labels_nn = [labels[:n_train][elt] for elt in idxs_sorted[nn:]] # the most elements with greater similarity are the closest\n",
    "        # select most frequent label as prediction\n",
    "        counts = dict(Counter(labels_nn))\n",
    "        max_counts = max(list(counts.values()))\n",
    "        pred = [k for k,v in counts.items() if v==max_counts][0]\n",
    "        preds_tfidf.append(pred)\n",
    "\n",
    "    # compare predictions to true labels\n",
    "    \n",
    "    print('accuracy for',nn,'nearest neighbors:',accuracy_score(labels[-n_test:],preds_tfidf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
