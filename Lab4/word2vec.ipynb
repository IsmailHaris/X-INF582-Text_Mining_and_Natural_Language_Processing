{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== init ==========\n",
    "\n",
    "path_root = \"\"\n",
    "\n",
    "path_to_data = path_root + 'data/'\n",
    "path_to_documents = path_root + 'data/documents/'\n",
    "path_to_plots = path_root\n",
    "path_to_google_news = path_root\n",
    "\n",
    "# remove dashes and apostrophes from punctuation marks \n",
    "punct = string.punctuation.replace('-', '').replace(\"'\",'')\n",
    "# regex to match intra-word dashes and intra-word apostrophes\n",
    "my_regex = re.compile(r\"(\\b[-']\\b)|[\\W_]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== functions ==========\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "# returns the vector of a word\n",
    "def my_vector_getter(word, wv):\n",
    "    try:\n",
    "        # we use reshape because cosine similarity in sklearn now works only for multidimensional arrays\n",
    "        word_array = wv.wv[word].reshape(1,-1)\n",
    "        return (word_array)\n",
    "    except KeyError:\n",
    "        print('word: <', word, '> not in vocabulary!')\n",
    "\n",
    "# returns cosine similarity between two word vectors\n",
    "def my_cos_similarity(word1, word2, wv):\n",
    "    sim = cosine(my_vector_getter(word1,wv), my_vector_getter(word2,wv))\n",
    "    return round(float(sim),4)\n",
    "\n",
    "# performs basic pre-processing\n",
    "def clean_string(my_str, punct=punct, my_regex=my_regex, to_lower=False):\n",
    "    if to_lower:\n",
    "        my_str = my_str.lower()\n",
    "    # remove formatting\n",
    "    my_str = re.sub('\\s+', ' ', my_str)\n",
    "     # remove punctuation\n",
    "    my_str = ''.join(l for l in my_str if l not in punct)\n",
    "    # remove dashes that are not intra-word\n",
    "    my_str = my_regex.sub(lambda x: (x.group(1) if x.group(1) else ' '), my_str)\n",
    "    # strip extra white space\n",
    "    my_str = re.sub(' +',' ', my_str)\n",
    "    # strip leading and trailing white space\n",
    "    my_str = my_str.strip()\n",
    "    return my_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1131\n",
      "2262\n",
      "3393\n",
      "4524\n",
      "5655\n",
      "6786\n",
      "7917\n",
      "9048\n",
      "10179\n",
      "11310\n",
      "documents loaded\n",
      "0\n",
      "1131\n",
      "2262\n",
      "3393\n",
      "4524\n",
      "5655\n",
      "6786\n",
      "7917\n",
      "9048\n",
      "10179\n",
      "11310\n",
      "documents cleaned\n"
     ]
    }
   ],
   "source": [
    "# ========== load and preprocess documents ==========\n",
    "\n",
    "with open(path_to_data + 'smart_stopwords.txt', 'r') as my_file: \n",
    "    stpwds = my_file.read().splitlines()\n",
    "\n",
    "doc_names = os.listdir(path_to_documents)\n",
    "doc_names.sort(key=natural_keys)\n",
    "docs = []\n",
    "for idx,name in enumerate(doc_names):\n",
    "    with open(path_to_documents + name,'r') as my_file:\n",
    "        docs.append(my_file.read())\n",
    "    if idx % round(len(doc_names)/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('documents loaded')\n",
    "\n",
    "cleaned_docs = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    # clean\n",
    "    doc = clean_string(doc, punct, my_regex, to_lower=True)\n",
    "    # tokenize (split based on whitespace)\n",
    "    tokens = doc.split(' ')\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stpwds]\n",
    "    # remove digits\n",
    "    tokens = [''.join([elt for elt in token if not elt.isdigit()]) for token in tokens]\n",
    "    # remove tokens shorter than 3 characters in size\n",
    "    tokens = [token for token in tokens if len(token)>2]\n",
    "    # remove tokens exceeding 25 characters in size\n",
    "    tokens = [token for token in tokens if len(token)<=25]\n",
    "    cleaned_docs.append(tokens)\n",
    "    if idx % round(len(docs)/10) == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('documents cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab built\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d6664f5e5eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# load vectors corresponding to our vocabulary (takes 1-2 mins)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_google_news\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word vectors loaded'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mintersect_word2vec_format\u001b[1;34m(self, fname, lockf, binary, encoding, unicode_errors)\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[0moverlap_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1063\u001b[0m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mbinary_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m     \u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mdecompressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msmart_open_s3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUPPORTED_SCHEMES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# ========== load Google News word vectors ==========\n",
    "\n",
    "# create empty word vectors for the words in vocabulary \n",
    "my_q = 300 # to match dim of GNews word vectors\n",
    "mcount = 5\n",
    "w2v = Word2Vec(size=my_q, min_count=mcount)\n",
    "\n",
    "# Use the build_vocab() method on our cleaned documents\n",
    "w2v.build_vocab(cleaned_docs)\n",
    "print('vocab built')\n",
    "\n",
    "# load vectors corresponding to our vocabulary (takes 1-2 mins)\n",
    "w2v.intersect_word2vec_format(path_to_google_news + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "print('word vectors loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== experiment with similarities in embedding space ==========\n",
    "\n",
    "# two similar words\n",
    "print('cosine similarity man/woman:', my_cos_similarity('man','woman',w2v))\n",
    "\n",
    "# two dissimilar words\n",
    "print('cosine similarity man/road:', my_cos_similarity('man','road',w2v))\n",
    "\n",
    "# examples of concepts captured in the embedding space:\n",
    "\n",
    "# gender\n",
    "queen = my_vector_getter('queen',w2v)\n",
    "king = my_vector_getter('king',w2v)\n",
    "man = my_vector_getter('man',w2v)\n",
    "woman = my_vector_getter('woman',w2v)\n",
    "\n",
    "operation = king - man + woman\n",
    "print('cosine similarity (king-man+woman)/queen:', round(float(cosine(operation, queen)),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== visualize word embeddings of 'n_mf' most frequent words ==========\n",
    "\n",
    "n_mf = 200\n",
    "all_tokens = [token for sublist in cleaned_docs for token in sublist]\n",
    "t_counts = dict(Counter(all_tokens))\n",
    "sorted_t_counts = sorted(list(t_counts.items()), key=operator.itemgetter(1), reverse=True)\n",
    "mft = [elt[0] for elt in sorted_t_counts[:n_mf]]\n",
    "\n",
    "# store the vectors of the most frequent words in np array\n",
    "mft_vecs = np.empty(shape=(n_mf,my_q))\n",
    "for idx,token in enumerate(mft):\n",
    "    # w2v.wv gives access to the word vectors\n",
    "    mft_vecs[idx,:] = w2v.wv[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca = PCA(n_components=10)\n",
    "my_tsne = TSNE(n_components=2,perplexity=10)\n",
    "\n",
    "mft_vecs_pca = my_pca.fit_transform(mft_vecs)\n",
    "mft_vecs_tsne = my_tsne.fit_transform(mft_vecs_pca)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(mft_vecs_tsne[:,0], mft_vecs_tsne[:,1],s=3)\n",
    "for x, y, token in zip(mft_vecs_tsne[:,0] , mft_vecs_tsne[:,1], mft):     \n",
    "    ax.annotate(token, xy=(x, y), size=8)\n",
    "fig.suptitle('t-SNE visualization of word embeddings',fontsize=20)\n",
    "fig.set_size_inches(11,7)\n",
    "fig.savefig(path_to_plots + 'word_embeddings.png',dpi=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== visualize regularities among word vectors ==========\n",
    "\n",
    "my_pca = PCA(n_components=2)\n",
    "# numpy array containg vectors of all words\n",
    "all_vecs = w2v.wv.syn0\n",
    "all_vecs_pca = my_pca.fit_transform(all_vecs) \n",
    "\n",
    "my_words = ['queen','king','woman','man','uncle','aunt','son','daughter']\n",
    "\n",
    "# get the row indexes of 'my_words' in 'all_vecs': w2v.wv.index2word is a list of words in the order in which they appear in w2v.wv.syn0\n",
    "idxs = [w2v.wv.index2word.index(word) for word in my_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(all_vecs_pca[idxs,0], all_vecs_pca[idxs,1],s=3)\n",
    "for x, y, token in zip(all_vecs_pca[idxs,0], all_vecs_pca[idxs,1], my_words):     \n",
    "    ax.annotate(token, xy=(x, y), size=8)\n",
    "fig.suptitle('PCA visualization of gender regularities',fontsize=15)\n",
    "fig.set_size_inches(7,5)\n",
    "fig.savefig(path_to_plots + 'regularities.png',dpi=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== document similarity ==========\n",
    "\n",
    "# 1. in vector space ('bag-of-words' representation)\n",
    "s_1 = 'computer science is the study of the design and use of computers'\n",
    "s_2 = 'programming language theory considers various approaches to the description of computation'\n",
    "\n",
    "s_1 = [elt for elt in s_1.split(' ') if elt not in stpwds]\n",
    "s_2 = [elt for elt in s_2.split(' ') if elt not in stpwds]\n",
    "\n",
    "print(s_1)\n",
    "print(s_2)\n",
    "\n",
    "features = list(set(s_1).union(set(s_2)))\n",
    "\n",
    "# project the two sentences in the vector space\n",
    "p_1 = [1 if feature in s_1 else 0 for feature in features]\n",
    "p_2 = [1 if feature in s_2 else 0 for feature in features]\n",
    "p_1_print = list(zip(features, p_1))\n",
    "p_2_print = list(zip(features, p_2))\n",
    "print('=== vector space representation of sentence 1: ===')\n",
    "print(p_1_print)\n",
    "print('=== vector space representation of sentence 2: ===')\n",
    "print(p_2_print)\n",
    "\n",
    "print('similarity of sentences 1 and 2 in vector space:', round(float(cosine(np.array(p_1).reshape(1,-1),np.array(p_2).reshape(1,-1))),5))                \n",
    "\n",
    "# 2. in word embeddings space \n",
    "p_1_embeddings = np.concatenate([my_vector_getter(token,w2v) for token in s_1])\n",
    "p_2_embeddings = np.concatenate([my_vector_getter(token,w2v) for token in s_2])\n",
    "\n",
    "# naive approach: centroids\n",
    "centroid_1 = np.mean(p_1_embeddings, axis=0).reshape(1,-1)\n",
    "centroid_2 = np.mean(p_2_embeddings, axis=0).reshape(1,-1)\n",
    "\n",
    "print('similarity of centroids of sentences 1 and 2 in word embeddings space:', round(float(cosine(centroid_1, centroid_2)),5))\n",
    "\n",
    "# more principled approach: Word Mover's Distance (Kusner et al. 2015)\n",
    "print('WMD between sentences 1 and 2:', w2v.wv.wmdistance(s_1,s_2))\n",
    "print('WMD between sentence 1 and itself:', w2v.wv.wmdistance(s_1,s_1))\n",
    "print('WMD between sentence 1 and \"the cat sat on the mat\":', w2v.wv.wmdistance(s_1, \"the cat sat on the mat\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
