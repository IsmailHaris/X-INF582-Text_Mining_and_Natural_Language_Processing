{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 7 - Report\n",
    "===\n",
    "\n",
    "**Antoine Carossio**\n",
    "\n",
    "NB: PLEASE READ THE JUPYTER NOTEBOOK VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('brown')\n",
    "from nltk import FreqDist\n",
    "from nltk.tag import HiddenMarkovModelTrainer\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import ConditionalProbDist, ConditionalFreqDist, LidstoneProbDist\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "from viterbi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ids(main_set, processed_set):\n",
    "    \"\"\"\n",
    "    Get the IDs of the elements from the processed_set into the main_set as a list\n",
    "    \"\"\"\n",
    "    return [main_set.index(element) for element in processed_set]\n",
    "\n",
    "def lists_to_array(a):\n",
    "    \"\"\"\n",
    "    Convert a list of lists to numpy array, padded with 0s if needed\n",
    "    \"\"\"\n",
    "    b = np.zeros([len(a),len(max(a,key = lambda x: len(x)))])\n",
    "    for i,j in enumerate(a):\n",
    "        b[i][0:len(j)] = j\n",
    "    return b\n",
    "\n",
    "import itertools\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten at list of lists to a lists\n",
    "    \"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a simple HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(processed_corpus):\n",
    "    \"\"\"\n",
    "    Extract words and tags form the corpus\n",
    "    \"\"\"\n",
    "    all_tags = []\n",
    "    all_words = []\n",
    "\n",
    "    for sent in processed_corpus:\n",
    "        for word, tag in sent:\n",
    "            all_words.append(word)\n",
    "            all_tags.append(tag)\n",
    "    \n",
    "    return all_words, all_tags\n",
    "\n",
    "def lidstone_cond_freq(processed_freq, norm_len, k=.1):\n",
    "    \"\"\"\n",
    "    Apply Lidstone to a ConditionalFreq() object\n",
    "    \"\"\"\n",
    "    \n",
    "    factory = lambda fd: LidstoneProbDist(fd, k, norm_len)\n",
    "    return ConditionalProbDist(processed_freq, factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "corpus = brown.tagged_sents()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 472\n"
     ]
    }
   ],
   "source": [
    "all_words, all_tags = extract_data(corpus)\n",
    "\n",
    "Q = list(set(all_tags))\n",
    "V = list(set(all_words))\n",
    "\n",
    "n = len(Q)\n",
    "m = len(V)\n",
    "\n",
    "A  = np.zeros((n,n))\n",
    "B  = np.zeros((n,m))\n",
    "Pi = np.zeros(n)\n",
    "\n",
    "print(\"Number of states: {}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the corpus into a training set and a testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = corpus[:-10]\n",
    "testing  = corpus[-10:]\n",
    "\n",
    "words_train, tags_train = extract_data(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the frequency conditional of nltk bigrams, and use the lidstone smoothing (with k=0.1) to estimate the conditional probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IN', 1222), ('NNS', 2357), ('NN-TL', 29), ('NN', 2669), ('NP', 89), ('JJ', 742), ('.', 236), ('NR', 24), ('JJT', 5), (\"'\", 4)]\n",
      "0.2804160537928136\n",
      "0.27904278007778194\n"
     ]
    }
   ],
   "source": [
    "tag_bigrams_freq_train = ConditionalFreqDist(nltk.bigrams(tags_train))\n",
    "tags_bigrams_prob_train = lidstone_cond_freq(tag_bigrams_freq_train, n)\n",
    "\n",
    "print(list(tag_bigrams_freq_train[\"AP\"].items())[:10]) # Print only the first 10 elements\n",
    "print(tag_bigrams_freq_train[\"AP\"].freq(\"NN\"))\n",
    "print(tags_bigrams_prob_train[\"AP\"].prob(\"NN\"))\n",
    "# Note : # tag_bigrams_freq_train[\"AP\"] == tags_bigrams_prob_train[\"AP\"].freqdist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999895\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A[i][j] = tags_bigrams_prob_train[Q[i]].prob(Q[j])\n",
    "print(sum(A[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AT', 8296), ('``', 4168), ('PPS', 5934), ('NN-HL', 431), ('WRB', 843), ('NN', 1377), ('RB', 3797), ('NNS-HL', 150), ('IN', 4791), ('VB-HL', 16)]\n",
      "0.9999999999999951\n"
     ]
    }
   ],
   "source": [
    "tags_first_train = [sent[0][1] for sent in training]\n",
    "tags_first_freq_train = FreqDist(tags_first_train)\n",
    "tags_first_prob_train = LidstoneProbDist(tags_first_freq_train, .1, n)\n",
    "\n",
    "print(list(tags_first_freq_train.items())[:10])\n",
    "\n",
    "for i in range(n):\n",
    "    Pi[i] = tags_first_prob_train.prob(Q[i])\n",
    "print(sum(Pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f904d263e6e40328ed394c81fe18f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=472), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9999999999995219\n"
     ]
    }
   ],
   "source": [
    "observations_freq_train = ConditionalFreqDist(zip(words_train, tags_train))\n",
    "observations_prob_train = lidstone_cond_freq(observations_freq_train, m)\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    for j in range(m):\n",
    "        B[i][j] = observations_prob_train[Q[i]].prob(V[j])\n",
    "print(sum(B[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d75a148137449568367224b12f2404f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tags_true = []\n",
    "tags_pred = []\n",
    "scores    = []\n",
    "\n",
    "for sent in tqdm(testing):\n",
    "    word_list = to_ids(V, [word for word, _ in sent])\n",
    "    tag_list  = to_ids(Q, [tag for _, tag in sent])\n",
    "    tags_true.append(tag_list)\n",
    "\n",
    "    predicted, score = viterbi((Pi,A,B), word_list)\n",
    "    tags_pred.append(predicted)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.16326530612244897\n",
      "Recall    : 0.8888888888888888\n",
      "F1-score  : 0.2758620689655172\n"
     ]
    }
   ],
   "source": [
    "predicted_set = set(flatten(tags_pred))\n",
    "reference_set = set(flatten(tags_true))\n",
    "\n",
    "print('Precision :', precision(predicted_set, reference_set))\n",
    "print('Recall    :', recall(predicted_set, reference_set))\n",
    "print('F1-score  :', f_measure(predicted_set, reference_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only conserve the pairs of tags which appears at least once in the whole set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_tri = list(set(nltk.bigrams(all_tags)))\n",
    "n_tri = len(Q_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute the Matrix $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN-TL': 545, 'NP-TL': 79, 'JJ-TL': 56, 'NNS-TL': 47, 'CC-TL': 27, 'NN': 8, '.': 7, 'CD-TL': 6, 'IN-TL': 3, 'FW-NN-TL': 3, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_trigrams_freq_train = ConditionalFreqDist(((w0, w1), w2) for w0, w1, w2 in nltk.trigrams(tags_train))\n",
    "tags_trigrams_prob_train = lidstone_cond_freq(tag_trigrams_freq_train, n)\n",
    "tags_trigrams_prob_train[('AT', 'NP-TL')].freqdist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6091b901eace4441a383e2940aad94cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.0000000000000129\n"
     ]
    }
   ],
   "source": [
    "A_tri = np.zeros((n_tri,n))\n",
    "\n",
    "for i in tqdm(range(n_tri)):\n",
    "    for j in range(n):\n",
    "        A_tri[i][j] = tags_trigrams_prob_train[Q_tri[i]].prob(Q[j])\n",
    "print(sum(A_tri[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83952ec17a2e44c4b8d8dc1f77f9fa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tags_true_tri = []\n",
    "tags_pred_tri = []\n",
    "scores_tri    = []\n",
    "\n",
    "for sent in tqdm(testing):\n",
    "    word_list = to_ids(V, [word for word, _ in sent])\n",
    "    tag_list  = to_ids(Q, [tag for _, tag in sent])\n",
    "    tags_true_tri.append(tag_list)\n",
    "\n",
    "    predicted, score = viterbi((Pi,A_tri,B), word_list)\n",
    "    tags_pred_tri.append(predicted)\n",
    "    scores_tri.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.14285714285714285\n",
      "Recall    : 0.7777777777777778\n",
      "F1-score  : 0.24137931034482762\n"
     ]
    }
   ],
   "source": [
    "predicted_set_tri = set(flatten(tags_pred_tri))\n",
    "reference_set_tri = set(flatten(tags_true_tri))\n",
    "\n",
    "print('Precision :', precision(predicted_set_tri, reference_set_tri))\n",
    "print('Recall    :', recall(predicted_set_tri, reference_set_tri))\n",
    "print('F1-score  :', f_measure(predicted_set_tri, reference_set_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using NLTK’s HMM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_train_tri = list(set(nltk.bigrams(tags_train)))\n",
    "observations_freq_train_tri = ConditionalFreqDist(zip(words_train, tags_train_tri))\n",
    "observations_prob_train_tri = lidstone_cond_freq(observations_freq_train_tri, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HiddenMarkovModelTrainer(all_tags, all_words)\n",
    "hmm = trainer.train_supervised(training, estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9804d4fe3f4adaa7f15f0069d0c767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags_true_lib = []\n",
    "tags_pred_lib = []\n",
    "\n",
    "for sent in tqdm(testing):\n",
    "    word_list = [word for word, _ in sent]\n",
    "    tag_list  = [tag for _, tag in sent]\n",
    "    tags_true_lib.append(tag_list)\n",
    "\n",
    "    tag_pred = hmm.tag(word_list)\n",
    "    tags_pred_lib.append(el[1] for el in tag_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.7346938775510204\n",
      "Recall    : 1.0\n",
      "F1-score  : 0.8470588235294119\n"
     ]
    }
   ],
   "source": [
    "predicted_set_lib = set(flatten(tags_pred_lib))\n",
    "reference_set_lib = set(flatten(tags_true_lib))\n",
    "\n",
    "print('Precision :', precision(predicted_set_lib, reference_set_lib))\n",
    "print('Recall    :', recall(predicted_set_lib, reference_set_lib))\n",
    "print('F1-score  :', f_measure(predicted_set_lib, reference_set_lib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have guessed easily it is better to used the HMM of the NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NER using Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "from crf_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n",
    "\n",
    "words = list(set(data[\"Word\"].values)) #vocabulary V\n",
    "tags = list(set(data[\"Tag\"].values)) #vocabulary V\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('demonstrators', 'NNS', 'O'),\n",
      " ('have', 'VBP', 'O'),\n",
      " ('marched', 'VBN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(data) #transform sentences into sequences of (Word, POS, Tag)\n",
    "sentences = getter.sentences\n",
    "\n",
    "pprint(sentences[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'+1:postag': 'IN',\n",
      "  '+1:word.lower()': 'of',\n",
      "  'BOS': True,\n",
      "  'bias': 1.0,\n",
      "  'postag': 'NNS',\n",
      "  'word.lower()': 'thousands'},\n",
      " {'+1:postag': 'NNS',\n",
      "  '+1:word.lower()': 'demonstrators',\n",
      "  '-1:postag': 'NNS',\n",
      "  '-1:word.lower()': 'thousands',\n",
      "  'bias': 1.0,\n",
      "  'postag': 'IN',\n",
      "  'word.lower()': 'of'},\n",
      " {'+1:postag': 'VBP',\n",
      "  '+1:word.lower()': 'have',\n",
      "  '-1:postag': 'IN',\n",
      "  '-1:word.lower()': 'of',\n",
      "  'bias': 1.0,\n",
      "  'postag': 'NNS',\n",
      "  'word.lower()': 'demonstrators'}]\n",
      "['O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "pprint(X[0][:3])\n",
    "pprint(y[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00       141\n",
      "       B-eve       0.61      0.27      0.38        92\n",
      "       B-geo       0.83      0.90      0.86     12418\n",
      "       B-gpe       0.98      0.83      0.90      5327\n",
      "       B-nat       1.00      0.01      0.02        85\n",
      "       B-org       0.78      0.68      0.73      6585\n",
      "       B-per       0.82      0.80      0.81      5609\n",
      "       B-tim       0.94      0.83      0.88      6696\n",
      "       I-art       0.00      0.00      0.00       103\n",
      "       I-eve       0.44      0.17      0.25        80\n",
      "       I-geo       0.80      0.77      0.78      2423\n",
      "       I-gpe       0.89      0.28      0.42        61\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.78      0.77      0.77      5493\n",
      "       I-per       0.83      0.91      0.87      5662\n",
      "       I-tim       0.83      0.70      0.76      2156\n",
      "           O       0.99      0.99      0.99    293715\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    346667\n",
      "   macro avg       0.68      0.52      0.55    346667\n",
      "weighted avg       0.96      0.97      0.96    346667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(algorithm='lbfgs', max_iterations=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=22)\n",
    "crf.fit(X_train, y_train)\n",
    "pred = crf.predict(X_test)\n",
    "report = flat_classification_report(y_pred = pred, y_true = y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a) CRF Equivalent to bigram HHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hmm = []\n",
    "hmm_features = [\"-1:postag\", \"postag\"]\n",
    "\n",
    "for sent in tqdm(X):\n",
    "    sent_words = []\n",
    "    for word in sent:\n",
    "        word_hmm_features = dict()\n",
    "        for feature in hmm_features:\n",
    "            if feature in word:\n",
    "                word_hmm_features[feature] = word[feature]\n",
    "        sent_words.append(word_hmm_features)\n",
    "    X_hmm.append(sent_words)\n",
    "    \n",
    "pprint(X_hmm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00       141\n",
      "       B-eve       0.00      0.00      0.00        92\n",
      "       B-geo       0.57      0.83      0.67     12418\n",
      "       B-gpe       0.81      0.05      0.09      5327\n",
      "       B-nat       0.00      0.00      0.00        85\n",
      "       B-org       0.59      0.25      0.35      6585\n",
      "       B-per       0.61      0.58      0.60      5609\n",
      "       B-tim       0.64      0.31      0.42      6696\n",
      "       I-art       0.00      0.00      0.00       103\n",
      "       I-eve       0.00      0.00      0.00        80\n",
      "       I-geo       0.47      0.37      0.42      2423\n",
      "       I-gpe       0.00      0.00      0.00        61\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.50      0.50      0.50      5493\n",
      "       I-per       0.59      0.84      0.69      5662\n",
      "       I-tim       0.60      0.31      0.41      2156\n",
      "           O       0.97      0.99      0.98    293715\n",
      "\n",
      "   micro avg       0.91      0.91      0.91    346667\n",
      "   macro avg       0.37      0.30      0.30    346667\n",
      "weighted avg       0.91      0.91      0.90    346667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf_hmm = CRF(algorithm='lbfgs', max_iterations=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hmm, y, test_size=0.33, random_state=22)\n",
    "crf_hmm.fit(X_train, y_train)\n",
    "pred = crf_hmm.predict(X_test)\n",
    "report = flat_classification_report(y_pred = pred, y_true = y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b) Features to improve the results for PER, GEO and ORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's take a look of words tagged by `B-geo`, `B-org`, `B-per`, `I-geo`, `I-org` and `I-per`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(label):\n",
    "    all_words = flatten(sentences)\n",
    "    res = []\n",
    "    for i in range(1,len(all_words)-1):\n",
    "        if all_words[i][2] == label:\n",
    "            res.append([all_words[i-1], all_words[i], all_words[i+1]])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bgeo_sents = get_context(\"B-geo\") # Geographical locations\n",
    "Borg_sents = get_context(\"B-org\") # Organisations\n",
    "Bper_sents = get_context(\"B-per\") # Persons\n",
    "Igeo_sents = get_context(\"I-geo\") # Geographical locations\n",
    "Iorg_sents = get_context(\"I-org\") # Organisations\n",
    "Iper_sents = get_context(\"I-per\") # Persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O')],\n",
      " [('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O')],\n",
      " [('the', 'DT', 'O'), ('Orakzai', 'NNP', 'B-geo'), ('tribal', 'JJ', 'O')],\n",
      " [('nearby', 'JJ', 'O'),\n",
      "  ('South', 'NNP', 'B-geo'),\n",
      "  ('Waziristan', 'NNP', 'I-geo')],\n",
      " [('.', '.', 'O'), ('U.N.', 'NNP', 'B-geo'), ('relief', 'NN', 'O')],\n",
      " [(',', ',', 'O'), ('U.S.', 'NNP', 'B-geo'), (',', ',', 'O')],\n",
      " [('western', 'JJ', 'O'), ('Aceh', 'NNP', 'B-geo'), ('province', 'NN', 'O')],\n",
      " [('the', 'DT', 'O'), ('Maldives', 'NNP', 'B-geo'), ('and', 'CC', 'O')],\n",
      " [('and', 'CC', 'O'), ('India', 'NNP', 'B-geo'), ('.', '.', 'O')],\n",
      " [('in', 'IN', 'O'), ('Asia', 'NNP', 'B-geo'), ('and', 'CC', 'O')]]\n",
      "\n",
      "[[('an', 'DT', 'O'), ('IAEA', 'NNP', 'B-org'), ('surveillance', 'NN', 'O')],\n",
      " [('many', 'JJ', 'O'), ('Taliban', 'NNP', 'B-org'), ('militants', 'NNS', 'O')],\n",
      " [('the', 'DT', 'O'), ('United', 'NNP', 'B-org'), ('Nations', 'NNP', 'I-org')],\n",
      " [('left', 'VBN', 'O'), ('Iceland', 'NNP', 'B-org'), (\"'s\", 'POS', 'O')],\n",
      " [('the', 'DT', 'O'),\n",
      "  ('Microsoft', 'NNP', 'B-org'),\n",
      "  ('Corporation', 'NNP', 'I-org')],\n",
      " [('agency', 'NN', 'O'), ('KCNA', 'NNP', 'B-org'), ('reported', 'VBD', 'O')],\n",
      " [('off', 'IN', 'O'), ('Indonesia', 'NNP', 'B-org'), (\"'s\", 'POS', 'I-org')],\n",
      " [('The', 'DT', 'O'), ('U.S.', 'NNP', 'B-org'), ('Geological', 'NNP', 'I-org')],\n",
      " [('of', 'IN', 'O'), ('Nias', 'NNP', 'B-org'), ('.', '.', 'O')],\n",
      " [('on', 'IN', 'O'), ('Nias', 'NNP', 'B-org'), ('.', '.', 'O')]]\n",
      "\n",
      "[[('coordinator', 'NN', 'O'),\n",
      "  ('Jan', 'NNP', 'B-per'),\n",
      "  ('Egeland', 'NNP', 'I-per')],\n",
      " [('.', '.', 'O'), ('Mr.', 'NNP', 'B-per'), ('Egeland', 'NNP', 'I-per')],\n",
      " [(',', ',', 'O'), ('Sri', 'NNP', 'B-per'), ('Lanka', 'NNP', 'B-gpe')],\n",
      " [(',', ',', 'O'), ('Prime', 'NNP', 'B-per'), ('Minister', 'NNP', 'O')],\n",
      " [('Minister', 'NNP', 'O'),\n",
      "  ('Fouad', 'NNP', 'B-per'),\n",
      "  ('Siniora', 'NNP', 'I-per')],\n",
      " [('former', 'JJ', 'O'), ('Prime', 'NNP', 'B-per'), ('Minister', 'NNP', 'O')],\n",
      " [('Minister', 'NNP', 'O'),\n",
      "  ('Rafik', 'NNP', 'B-per'),\n",
      "  ('Hariri', 'NNP', 'I-per')],\n",
      " [('investigator', 'NN', 'O'),\n",
      "  ('Detlev', 'NNP', 'B-per'),\n",
      "  ('Mehlis', 'NNP', 'I-per')],\n",
      " [('say', 'VBP', 'O'), ('Prime', 'NNP', 'B-per'), ('Minister', 'NNP', 'I-per')],\n",
      " [('of', 'IN', 'O'), ('Mr.', 'NNP', 'B-per'), ('Sharon', 'NNP', 'I-per')]]\n",
      "\n",
      "[[('South', 'NNP', 'B-geo'), ('Waziristan', 'NNP', 'I-geo'), ('.', '.', 'O')],\n",
      " [('New', 'NNP', 'B-geo'), ('York', 'NNP', 'I-geo'), (',', ',', 'O')],\n",
      " [('North', 'NNP', 'B-geo'), ('Korea', 'NNP', 'I-geo'), ('says', 'VBZ', 'O')],\n",
      " [('North', 'NNP', 'B-geo'), ('Korea', 'NNP', 'I-geo'), ('left', 'VBD', 'O')],\n",
      " [('Heathrow', 'NNP', 'B-geo'), ('Airport', 'NNP', 'I-geo'), ('.', '.', 'O')],\n",
      " [('South', 'NNP', 'B-geo'), ('Africa', 'NNP', 'I-geo'), (',', ',', 'O')],\n",
      " [('Southern', 'NNP', 'B-geo'),\n",
      "  ('California', 'NNP', 'I-geo'),\n",
      "  (',', ',', 'O')],\n",
      " [('United', 'NNP', 'B-geo'), ('States', 'NNPS', 'I-geo'), ('.', '.', 'O')],\n",
      " [('San', 'NNP', 'B-geo'), ('Suu', 'NNP', 'I-geo'), ('Kyi', 'NNP', 'I-geo')],\n",
      " [('Suu', 'NNP', 'I-geo'), ('Kyi', 'NNP', 'I-geo'), ('is', 'VBZ', 'O')]]\n",
      "\n",
      "[[('United', 'NNP', 'B-org'),\n",
      "  ('Nations', 'NNP', 'I-org'),\n",
      "  ('summit', 'NN', 'O')],\n",
      " [('Microsoft', 'NNP', 'B-org'),\n",
      "  ('Corporation', 'NNP', 'I-org'),\n",
      "  ('.', '.', 'O')],\n",
      " [('Indonesia', 'NNP', 'B-org'),\n",
      "  (\"'s\", 'POS', 'I-org'),\n",
      "  ('Sumatra', 'NNP', 'I-org')],\n",
      " [(\"'s\", 'POS', 'I-org'), ('Sumatra', 'NNP', 'I-org'), ('and', 'CC', 'I-org')],\n",
      " [('Sumatra', 'NNP', 'I-org'),\n",
      "  ('and', 'CC', 'I-org'),\n",
      "  ('Nias', 'NNP', 'I-org')],\n",
      " [('and', 'CC', 'I-org'),\n",
      "  ('Nias', 'NNP', 'I-org'),\n",
      "  ('islands', 'NNS', 'I-org')],\n",
      " [('Nias', 'NNP', 'I-org'), ('islands', 'NNS', 'I-org'), ('has', 'VBZ', 'O')],\n",
      " [('U.S.', 'NNP', 'B-org'),\n",
      "  ('Geological', 'NNP', 'I-org'),\n",
      "  ('Survey', 'NNP', 'I-org')],\n",
      " [('Geological', 'NNP', 'I-org'),\n",
      "  ('Survey', 'NNP', 'I-org'),\n",
      "  ('gave', 'VBD', 'O')],\n",
      " [('UN', 'NNP', 'B-org'),\n",
      "  ('Secretary-General', 'NNP', 'I-org'),\n",
      "  ('Kofi', 'NNP', 'B-per')]]\n",
      "\n",
      "[[('Jan', 'NNP', 'B-per'), ('Egeland', 'NNP', 'I-per'), ('said', 'VBD', 'O')],\n",
      " [('Mr.', 'NNP', 'B-per'), ('Egeland', 'NNP', 'I-per'), ('said', 'VBD', 'O')],\n",
      " [('Fouad', 'NNP', 'B-per'), ('Siniora', 'NNP', 'I-per'), ('said', 'VBD', 'O')],\n",
      " [('Rafik', 'NNP', 'B-per'), ('Hariri', 'NNP', 'I-per'), ('and', 'CC', 'O')],\n",
      " [('Detlev', 'NNP', 'B-per'), ('Mehlis', 'NNP', 'I-per'), ('is', 'VBZ', 'O')],\n",
      " [('Prime', 'NNP', 'B-per'),\n",
      "  ('Minister', 'NNP', 'I-per'),\n",
      "  ('Ariel', 'NNP', 'I-per')],\n",
      " [('Minister', 'NNP', 'I-per'),\n",
      "  ('Ariel', 'NNP', 'I-per'),\n",
      "  ('Sharon', 'NNP', 'I-per')],\n",
      " [('Ariel', 'NNP', 'I-per'), ('Sharon', 'NNP', 'I-per'), ('will', 'MD', 'O')],\n",
      " [('Mr.', 'NNP', 'B-per'), ('Sharon', 'NNP', 'I-per'), (\"'s\", 'POS', 'O')],\n",
      " [('Mr.', 'NNP', 'B-per'), ('Sharon', 'NNP', 'I-per'), ('will', 'MD', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "pprint(Bgeo_sents[:10])\n",
    "print()\n",
    "pprint(Borg_sents[:10])\n",
    "print()\n",
    "pprint(Bper_sents[:10])\n",
    "print()\n",
    "pprint(Igeo_sents[:10])\n",
    "print()\n",
    "pprint(Iorg_sents[:10])\n",
    "print()\n",
    "pprint(Iper_sents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at some samples of each tags, we can see that some new features be could revelant to add, such ad:\n",
    "- Does the word starts with a capital letter?\n",
    "- Does the previous word starts with a capital letter? (especially for `B-per`)\n",
    "- What is the total number of capitals in the word?\n",
    "- What is the total number of dots in the word ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with_capital(word):\n",
    "    return int(word[0].isupper())\n",
    "\n",
    "def number_of_capitals(word):\n",
    "    return sum(letter.isupper() for letter in word)\n",
    "\n",
    "def number_of_dots(word):\n",
    "    return word.count(\".\")\n",
    "\n",
    "def word2features_plus(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = { #features related to the current position\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    \n",
    "    ## +1 and +2 words\n",
    "    if i > 0: #features related to preceding word/tag\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:starts_with_capital': number_of_capitals(word1)       # NEW\n",
    "        })\n",
    "        if i > 1:\n",
    "            word2 = sent[i-2][0]\n",
    "            postag2 = sent[i-2][1]\n",
    "            features.update({\n",
    "                '-2:word.lower()': word2.lower(),                     # NEW\n",
    "                '-2:start_with_capital' : starts_with_capital(word2), # NEW\n",
    "                '-2:postag': postag2,                                 # NEW\n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True #feature for Beginning of Sentence\n",
    "\n",
    "    ## -1 and -2 words\n",
    "    if i < len(sent)-1: #features related to the following word/tag\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:start_with_capital' : starts_with_capital(word1),     # NEW\n",
    "        })\n",
    "        if i < len(sent)-2:\n",
    "            word2 = sent[i+2][0]\n",
    "            postag2 = sent[i+2][1]\n",
    "            features.update({\n",
    "                '+2:word.lower()': word2.lower(),                     # NEW\n",
    "                '+2:postag': postag2,                                 # NEW\n",
    "                '+2:start_with_capital' : starts_with_capital(word2), # NEW\n",
    "            })\n",
    "    else:\n",
    "        features['EOS'] = True #feature for end of sentence\n",
    "\n",
    "    features.update({\n",
    "        'starts_with_capital': starts_with_capital(word),             # NEW\n",
    "        'number_of_capitals': number_of_capitals(word),               # NEW\n",
    "        'number_of_dots': number_of_dots(word)                        # NEW\n",
    "    })\n",
    "\n",
    "    return features\n",
    "\n",
    "#transform the sentence in a sequence of features\n",
    "def sent2features_plus(sent):\n",
    "    return [word2features_plus(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2870812a474b4e0289b7c7eac45ecfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47959), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_plus = [sent2features_plus(s) for s in tqdm(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00       141\n",
      "       B-eve       0.54      0.28      0.37        92\n",
      "       B-geo       0.84      0.89      0.87     12418\n",
      "       B-gpe       0.94      0.91      0.92      5327\n",
      "       B-nat       0.50      0.01      0.02        85\n",
      "       B-org       0.74      0.72      0.73      6585\n",
      "       B-per       0.84      0.78      0.81      5609\n",
      "       B-tim       0.93      0.83      0.88      6696\n",
      "       I-art       0.00      0.00      0.00       103\n",
      "       I-eve       0.42      0.20      0.27        80\n",
      "       I-geo       0.80      0.75      0.78      2423\n",
      "       I-gpe       0.83      0.49      0.62        61\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.73      0.81      0.77      5493\n",
      "       I-per       0.84      0.88      0.86      5662\n",
      "       I-tim       0.80      0.71      0.75      2156\n",
      "           O       0.99      0.99      0.99    293715\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    346667\n",
      "   macro avg       0.63      0.55      0.57    346667\n",
      "weighted avg       0.97      0.97      0.97    346667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf_plus = CRF(algorithm='lbfgs', max_iterations=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_plus, y, test_size=0.33, random_state=22)\n",
    "crf_plus.fit(X_train, y_train)\n",
    "pred = crf_plus.predict(X_test)\n",
    "report = flat_classification_report(y_pred = pred, y_true = y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reports comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 11 new features the F1-score is improved for all tags, and especially for the tags of interests. The results are summarized in the following table:\n",
    "\n",
    "| Tag   | HHM equivalent | Given CRF | CRF+ 11 features |\n",
    "|--|--|--|--|\n",
    "| B-geo | 0.67 | 0.86 | 0.87 |\n",
    "| B-per | 0.60 | 0.80 | 0.81 |\n",
    "| B-org | 0.35 | 0.73 | 0.75 |\n",
    "| I-geo | 0.42 | 0.78 | 0.79 |\n",
    "| I-per | 0.69 | 0.86 | 0.87 |\n",
    "| I-org | 0.50 | 0.77 | 0.79 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss # Change n°0 (to avoid a Deprecation Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'glove/' \n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 #if you use other embeddings, introduce the right size here\n",
    "max_words = n_words+1 # Change n°1\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "tag2idx  = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len=75\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words-1)\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(max_len,))\n",
    "model = Embedding(max_words, embedding_dim, input_length=max_len, mask_zero=True, weights=[embedding_matrix], trainable=False)(input_layer) # Change n°2\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model) # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
    "crf = CRF(n_tags)\n",
    "out = crf(model)\n",
    "\n",
    "model = Model(input_layer, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf_loss, metrics=[crf.accuracy])  # Change n°0 (to avoid a Deprecation Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21528 samples, validate on 10604 samples\n",
      "Epoch 1/5\n",
      "21528/21528 [==============================] - 86s 4ms/step - loss: 0.1351 - crf_viterbi_accuracy: 0.9609 - val_loss: 0.0715 - val_crf_viterbi_accuracy: 0.9744\n",
      "Epoch 2/5\n",
      "21528/21528 [==============================] - 82s 4ms/step - loss: 0.0642 - crf_viterbi_accuracy: 0.9758 - val_loss: 0.0581 - val_crf_viterbi_accuracy: 0.9772\n",
      "Epoch 3/5\n",
      "21528/21528 [==============================] - 82s 4ms/step - loss: 0.0546 - crf_viterbi_accuracy: 0.9778 - val_loss: 0.0522 - val_crf_viterbi_accuracy: 0.9782\n",
      "Epoch 4/5\n",
      "21528/21528 [==============================] - 82s 4ms/step - loss: 0.0500 - crf_viterbi_accuracy: 0.9787 - val_loss: 0.0491 - val_crf_viterbi_accuracy: 0.9788\n",
      "Epoch 5/5\n",
      "21528/21528 [==============================] - 82s 4ms/step - loss: 0.0472 - crf_viterbi_accuracy: 0.9793 - val_loss: 0.0477 - val_crf_viterbi_accuracy: 0.9790\n",
      "15827/15827 [==============================] - 15s 966us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=5, validation_split=0.33, verbose=1)\n",
    "test_pred = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results without Embedding:\n",
    "\n",
    "```bash=\n",
    "Train on 21528 samples, validate on 10604 samples\n",
    "Epoch 1/5\n",
    "21528/21528 [==============================] - 86s 4ms/step - loss: 0.1555 - crf_viterbi_accuracy: 0.9598 - val_loss: 0.0747 - val_crf_viterbi_accuracy: 0.9761\n",
    "Epoch 2/5\n",
    "21528/21528 [==============================] - 83s 4ms/step - loss: 0.0513 - crf_viterbi_accuracy: 0.9830 - val_loss: 0.0397 - val_crf_viterbi_accuracy: 0.9860\n",
    "Epoch 3/5\n",
    "21528/21528 [==============================] - 83s 4ms/step - loss: 0.0315 - crf_viterbi_accuracy: 0.9885 - val_loss: 0.0311 - val_crf_viterbi_accuracy: 0.9882\n",
    "Epoch 4/5\n",
    "21528/21528 [==============================] - 83s 4ms/step - loss: 0.0252 - crf_viterbi_accuracy: 0.9903 - val_loss: 0.0287 - val_crf_viterbi_accuracy: 0.9882\n",
    "Epoch 5/5\n",
    "21528/21528 [==============================] - 83s 4ms/step - loss: 0.0220 - crf_viterbi_accuracy: 0.9913 - val_loss: 0.0265 - val_crf_viterbi_accuracy: 0.9893\n",
    "15827/15827 [==============================] - 14s 871us/step\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
